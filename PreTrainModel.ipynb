{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a href=\"https://cognitiveclass.ai\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n\n<h1 align=center><font size = 5>Pre-Trained Models</font></h1>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Introduction\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Table of Contents\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n<font size = 3> \n    \n1. <a href=\"#item31\">Import Libraries and Packages</a>\n2. <a href=\"#item32\">Download Data</a>  \n3. <a href=\"#item33\">Define Global Constants</a>  \n4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n5. <a href=\"#item35\">Compile and Fit Model</a>\n\n</font>\n    \n</div>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "   "
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a id='item31'></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Import Libraries and Packages"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Let's start the lab by importing the libraries that we will be using in this lab."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches."
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Using TensorFlow backend.\n"
                }
            ],
            "source": "from keras.preprocessing.image import ImageDataGenerator"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library."
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "from keras.applications import ResNet50\nfrom keras.applications.resnet50 import preprocess_input"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a id='item32'></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Download Data"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "For your convenience, I have placed the data on a server which you can retrieve easily using the **wget** command. So let's run the following line of code to get the data. Given the large size of the image dataset, it might take some time depending on your internet speed."
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "## get the data\n#!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "And now if you check the left directory pane, you should see the zipped file *concrete_data_week3.zip* appear. So, let's go ahead and unzip the file to access the images. Given the large number of images in the dataset, this might take a couple of minutes, so please be patient, and wait until the code finishes running."
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                },
                "scrolled": true
            },
            "outputs": [],
            "source": "#!unzip concrete_data_week3.zip"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50*** error. So please **DO NOT DO IT**."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a id='item33'></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Define Global Constants"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Here, we will define constants that we will be using throughout the rest of the lab. \n\n1. We are obviously dealing with two classes, so *num_classes* is 2. \n2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n3. We will training and validating the model using batches of 100 images."
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "num_classes = 2\n\nimage_resize = 224\n\nbatch_size_training = 100\nbatch_size_validation = 100"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a id='item34'></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Construct ImageDataGenerator Instances"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed."
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "data_generator = ImageDataGenerator(\n    preprocessing_function=preprocess_input,\n)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Next, we will use the *flow_from_directory* method to get the training images as follows:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Found 30001 images belonging to 2 classes.\n"
                },
                {
                    "data": {
                        "text/plain": "<keras_preprocessing.image.DirectoryIterator at 0x7fc8216be048>"
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "train_generator = data_generator.flow_from_directory(\n    'concrete_data_week3/train',\n    target_size=(image_resize, image_resize),\n    batch_size=batch_size_training,\n    class_mode='categorical')\n\ntrain_generator"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Found 10001 images belonging to 2 classes.\n"
                },
                {
                    "data": {
                        "text/plain": "<keras_preprocessing.image.DirectoryIterator at 0x7fc8216d1518>"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "## Type your answer here\nvalidation_generator = data_generator.flow_from_directory(\n    'concrete_data_week3/valid',\n    target_size=(image_resize, image_resize),\n    batch_size=batch_size_validation,\n    class_mode='categorical')\n\nvalidation_generator\n\n\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Double-click __here__ for the solution.\n<!-- The correct answer is:\nvalidation_generator = data_generator.flow_from_directory(\n    'concrete_data_week3/valid',\n    target_size=(image_resize, image_resize),\n    batch_size=batch_size_validation,\n    class_mode='categorical')\n-->\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a id='item35'></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Build, Compile and Fit Model"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "In this section, we will start building our model. We will use the Sequential model class from Keras."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "model = Sequential()"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n"
                }
            ],
            "source": "model.add(ResNet50(\n    include_top=False,\n    pooling='avg',\n    weights='imagenet',\n    ))"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "model.add(Dense(num_classes, activation='softmax'))"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "You can access the model's layers using the *layers* attribute of our model object. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "[<keras.engine.training.Model at 0x7fc71453bef0>,\n <keras.layers.core.Dense at 0x7fc6782c00f0>]"
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model.layers"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "You can access the ResNet50 layers by running the following:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                },
                "scrolled": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "[<keras.engine.input_layer.InputLayer at 0x7fc82065b518>,\n <keras.layers.convolutional.ZeroPadding2D at 0x7fc82065ba90>,\n <keras.layers.convolutional.Conv2D at 0x7fc82065bc50>,\n <keras.layers.normalization.BatchNormalization at 0x7fc82065be80>,\n <keras.layers.core.Activation at 0x7fc82066a0f0>,\n <keras.layers.convolutional.ZeroPadding2D at 0x7fc8216be828>,\n <keras.layers.pooling.MaxPooling2D at 0x7fc8216beb70>,\n <keras.layers.convolutional.Conv2D at 0x7fc82068e710>,\n <keras.layers.normalization.BatchNormalization at 0x7fc8187822b0>,\n <keras.layers.core.Activation at 0x7fc818782cf8>,\n <keras.layers.convolutional.Conv2D at 0x7fc8187b6d30>,\n <keras.layers.normalization.BatchNormalization at 0x7fc8186fda90>,\n <keras.layers.core.Activation at 0x7fc8187239b0>,\n <keras.layers.convolutional.Conv2D at 0x7fc818631f60>,\n <keras.layers.convolutional.Conv2D at 0x7fc8185db128>,\n <keras.layers.normalization.BatchNormalization at 0x7fc81860aa90>,\n <keras.layers.normalization.BatchNormalization at 0x7fc8184c7828>,\n <keras.layers.merge.Add at 0x7fc818492ba8>,\n <keras.layers.core.Activation at 0x7fc8183c4e10>,\n <keras.layers.convolutional.Conv2D at 0x7fc8183e5320>,\n <keras.layers.normalization.BatchNormalization at 0x7fc81834f160>,\n <keras.layers.core.Activation at 0x7fc8182d8940>,\n <keras.layers.convolutional.Conv2D at 0x7fc8182a2160>,\n <keras.layers.normalization.BatchNormalization at 0x7fc81823db38>,\n <keras.layers.core.Activation at 0x7fc8182085c0>,\n <keras.layers.convolutional.Conv2D at 0x7fc8180a5278>,\n <keras.layers.normalization.BatchNormalization at 0x7fc8180fb3c8>,\n <keras.layers.merge.Add at 0x7fc8180e8198>,\n <keras.layers.core.Activation at 0x7fc7f47d3be0>,\n <keras.layers.convolutional.Conv2D at 0x7fc7f47b45f8>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7f4757390>,\n <keras.layers.core.Activation at 0x7fc7f4730ba8>,\n <keras.layers.convolutional.Conv2D at 0x7fc7f4668a90>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7f463d898>,\n <keras.layers.core.Activation at 0x7fc7f460c9b0>,\n <keras.layers.convolutional.Conv2D at 0x7fc7f45582e8>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7f4573828>,\n <keras.layers.merge.Add at 0x7fc7f44c3c88>,\n <keras.layers.core.Activation at 0x7fc7f4413668>,\n <keras.layers.convolutional.Conv2D at 0x7fc7f4413780>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7f4379f60>,\n <keras.layers.core.Activation at 0x7fc7f4378630>,\n <keras.layers.convolutional.Conv2D at 0x7fc7f42cb4e0>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7f42ae518>,\n <keras.layers.core.Activation at 0x7fc7f4255240>,\n <keras.layers.convolutional.Conv2D at 0x7fc7f4187eb8>,\n <keras.layers.convolutional.Conv2D at 0x7fc7f4107518>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7f41627f0>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7d4786978>,\n <keras.layers.merge.Add at 0x7fc7d47e0e10>,\n <keras.layers.core.Activation at 0x7fc7d46ef4a8>,\n <keras.layers.convolutional.Conv2D at 0x7fc7d4696b00>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7d463d908>,\n <keras.layers.core.Activation at 0x7fc7d45c40f0>,\n <keras.layers.convolutional.Conv2D at 0x7fc7d4545fd0>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7d45a4a90>,\n <keras.layers.core.Activation at 0x7fc7d4572dd8>,\n <keras.layers.convolutional.Conv2D at 0x7fc7d44442b0>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7d44272e8>,\n <keras.layers.merge.Add at 0x7fc7d43cd160>,\n <keras.layers.core.Activation at 0x7fc7d42ffb00>,\n <keras.layers.convolutional.Conv2D at 0x7fc7d42df5c0>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7d4286358>,\n <keras.layers.core.Activation at 0x7fc7d425fb70>,\n <keras.layers.convolutional.Conv2D at 0x7fc7d4197a58>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7d41f1860>,\n <keras.layers.core.Activation at 0x7fc7d413c978>,\n <keras.layers.convolutional.Conv2D at 0x7fc7d404cc50>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7d40a37f0>,\n <keras.layers.merge.Add at 0x7fc7d406fc50>,\n <keras.layers.core.Activation at 0x7fc7b46ff630>,\n <keras.layers.convolutional.Conv2D at 0x7fc7b46ff748>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7b46e9f28>,\n <keras.layers.core.Activation at 0x7fc7b46645f8>,\n <keras.layers.convolutional.Conv2D at 0x7fc7b45ba4a8>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7b459e4e0>,\n <keras.layers.core.Activation at 0x7fc7b4549208>,\n <keras.layers.convolutional.Conv2D at 0x7fc7b44f7e80>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7b44527b8>,\n <keras.layers.merge.Add at 0x7fc7b44784e0>,\n <keras.layers.core.Activation at 0x7fc7b4346d68>,\n <keras.layers.convolutional.Conv2D at 0x7fc7b4364438>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7b4333940>,\n <keras.layers.core.Activation at 0x7fc7b423a748>,\n <keras.layers.convolutional.Conv2D at 0x7fc7b41bdf28>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7b421b9e8>,\n <keras.layers.core.Activation at 0x7fc7b41e9e10>,\n <keras.layers.convolutional.Conv2D at 0x7fc7b40bc208>,\n <keras.layers.convolutional.Conv2D at 0x7fc7b40a5630>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7b40d8be0>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7946bb240>,\n <keras.layers.merge.Add at 0x7fc794717668>,\n <keras.layers.core.Activation at 0x7fc79460af60>,\n <keras.layers.convolutional.Conv2D at 0x7fc794625470>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7945f4978>,\n <keras.layers.core.Activation at 0x7fc79456d5f8>,\n <keras.layers.convolutional.Conv2D at 0x7fc7944c44e0>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7944a6518>,\n <keras.layers.core.Activation at 0x7fc79444d240>,\n <keras.layers.convolutional.Conv2D at 0x7fc794380eb8>,\n <keras.layers.normalization.BatchNormalization at 0x7fc79435b7f0>,\n <keras.layers.merge.Add at 0x7fc7942fc518>,\n <keras.layers.core.Activation at 0x7fc794250da0>,\n <keras.layers.convolutional.Conv2D at 0x7fc79426c470>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7941bc978>,\n <keras.layers.core.Activation at 0x7fc7941b45f8>,\n <keras.layers.convolutional.Conv2D at 0x7fc79410a4a8>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7940ed4e0>,\n <keras.layers.core.Activation at 0x7fc794097208>,\n <keras.layers.convolutional.Conv2D at 0x7fc774786e80>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7747617b8>,\n <keras.layers.merge.Add at 0x7fc7747064e0>,\n <keras.layers.core.Activation at 0x7fc774659d68>,\n <keras.layers.convolutional.Conv2D at 0x7fc774674438>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7745c3940>,\n <keras.layers.core.Activation at 0x7fc7745405c0>,\n <keras.layers.convolutional.Conv2D at 0x7fc774513470>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7744f54a8>,\n <keras.layers.core.Activation at 0x7fc77449c278>,\n <keras.layers.convolutional.Conv2D at 0x7fc7743cfe48>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7743a8780>,\n <keras.layers.merge.Add at 0x7fc77434f4a8>,\n <keras.layers.core.Activation at 0x7fc77429cf28>,\n <keras.layers.convolutional.Conv2D at 0x7fc77423c400>,\n <keras.layers.normalization.BatchNormalization at 0x7fc77420a908>,\n <keras.layers.core.Activation at 0x7fc774184588>,\n <keras.layers.convolutional.Conv2D at 0x7fc77415a438>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7740be470>,\n <keras.layers.core.Activation at 0x7fc7740e6240>,\n <keras.layers.convolutional.Conv2D at 0x7fc7547d7e10>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7547b0748>,\n <keras.layers.merge.Add at 0x7fc754759470>,\n <keras.layers.core.Activation at 0x7fc7546a4f28>,\n <keras.layers.convolutional.Conv2D at 0x7fc7546423c8>,\n <keras.layers.normalization.BatchNormalization at 0x7fc75460e8d0>,\n <keras.layers.core.Activation at 0x7fc75458c550>,\n <keras.layers.convolutional.Conv2D at 0x7fc75455f400>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7544c1438>,\n <keras.layers.core.Activation at 0x7fc7544e7208>,\n <keras.layers.convolutional.Conv2D at 0x7fc754419dd8>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7543f5710>,\n <keras.layers.merge.Add at 0x7fc75439d438>,\n <keras.layers.core.Activation at 0x7fc7542efeb8>,\n <keras.layers.convolutional.Conv2D at 0x7fc75428c390>,\n <keras.layers.normalization.BatchNormalization at 0x7fc754259898>,\n <keras.layers.core.Activation at 0x7fc7541dd518>,\n <keras.layers.convolutional.Conv2D at 0x7fc7541a93c8>,\n <keras.layers.normalization.BatchNormalization at 0x7fc75410a400>,\n <keras.layers.core.Activation at 0x7fc75412f1d0>,\n <keras.layers.convolutional.Conv2D at 0x7fc754060da0>,\n <keras.layers.convolutional.Conv2D at 0x7fc7347a2400>,\n <keras.layers.normalization.BatchNormalization at 0x7fc73477c6d8>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7346f5c88>,\n <keras.layers.merge.Add at 0x7fc73465e198>,\n <keras.layers.core.Activation at 0x7fc7345b0240>,\n <keras.layers.convolutional.Conv2D at 0x7fc7345b0668>,\n <keras.layers.normalization.BatchNormalization at 0x7fc734519c50>,\n <keras.layers.core.Activation at 0x7fc734498a58>,\n <keras.layers.convolutional.Conv2D at 0x7fc73446dac8>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7343cb978>,\n <keras.layers.core.Activation at 0x7fc7343ee6a0>,\n <keras.layers.convolutional.Conv2D at 0x7fc734280f60>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7342daa58>,\n <keras.layers.merge.Add at 0x7fc7342aab70>,\n <keras.layers.core.Activation at 0x7fc7341f7240>,\n <keras.layers.convolutional.Conv2D at 0x7fc7341f7668>,\n <keras.layers.normalization.BatchNormalization at 0x7fc734161c50>,\n <keras.layers.core.Activation at 0x7fc7340dfa58>,\n <keras.layers.convolutional.Conv2D at 0x7fc7340b48d0>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7147cf940>,\n <keras.layers.core.Activation at 0x7fc7147f6668>,\n <keras.layers.convolutional.Conv2D at 0x7fc714689f28>,\n <keras.layers.normalization.BatchNormalization at 0x7fc7146e2a20>,\n <keras.layers.merge.Add at 0x7fc7145dd9b0>,\n <keras.layers.core.Activation at 0x7fc71457d6a0>,\n <keras.layers.pooling.GlobalAveragePooling2D at 0x7fc71457d630>]"
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model.layers[0].layers"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "model.layers[0].trainable = False"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nresnet50 (Model)             (None, 2048)              23587712  \n_________________________________________________________________\ndense_1 (Dense)              (None, 2)                 4098      \n=================================================================\nTotal params: 23,591,810\nTrainable params: 4,098\nNon-trainable params: 23,587,712\n_________________________________________________________________\n"
                }
            ],
            "source": "model.summary()"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Next we compile our model using the **adam** optimizer."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "steps_per_epoch_training = len(train_generator)\nsteps_per_epoch_validation = len(validation_generator)\nnum_epochs = 2"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/2\n242/301 [=======================>......] - ETA: 29:16 - loss: 0.0502 - acc: 0.9834"
                }
            ],
            "source": "fit_history = model.fit_generator(\n    train_generator,\n    steps_per_epoch=steps_per_epoch_training,\n    epochs=num_epochs,\n    validation_data=validation_generator,\n    validation_steps=steps_per_epoch_validation,\n    verbose=1,\n)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Now that the model is trained, you are ready to start using it to classify images."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "model.save('classifier_resnet_model.h5')"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "### Thank you for completing this lab!\n\nThis notebook was created by Alex Aklson. I hope you found this lab interesting and educational."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1)."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<hr>\n\nCopyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/)."
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}